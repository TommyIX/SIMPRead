{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import math\r\n",
    "import flask\r\n",
    "import spacy\r\n",
    "import torch\r\n",
    "import itertools\r\n",
    "\r\n",
    "from model_ebldl import *\r\n",
    "from train import evaluate\r\n",
    "from PreProcess import *\r\n",
    "\r\n",
    "app = flask.Flask(__name__)\r\n",
    "# 各项参数设置=======================================================\r\n",
    "model_path = './model.pth'\r\n",
    "parser = spacy.load('en_core_web_trf')\r\n",
    "# =================================================================\r\n",
    "\r\n",
    "\r\n",
    "def load_best_model (device, model_path = './model.pth', store_dict = \"./data\"):\r\n",
    "    article = Field(tokenize='spacy', init_token='<sos>', eos_token='<eos>', lower=True, tokenizer_language='en_core_web_trf', include_lengths=True)\r\n",
    "    summary = Field(tokenize='spacy',init_token='<sos>',eos_token='<eos>',lower=True, tokenizer_language='en_core_web_trf')\r\n",
    "\r\n",
    "    train_data, valid_data, test_data = TabularDataset.splits(path=store_dict, train='train.csv', validation='val.csv', test='test.csv', format='csv', fields=[(\"text\",article),('headline',summary)])\r\n",
    "\r\n",
    "    article.build_vocab(train_data, min_freq=2)\r\n",
    "    summary.build_vocab(train_data, min_freq=2)\r\n",
    "\r\n",
    "    _, _, test_loader = BucketIterator.splits((train_data, valid_data, test_data), batch_size=64, sort_within_batch=True, sort_key = lambda x:len(x.text), device=device)\r\n",
    "\r\n",
    "    attention_layer = Attention(enc_hid_dim = 512, dec_hid_dim = 512)\r\n",
    "    encode_layer = Encoder(vocab=len(article.vocab),embeding_dim=256, encoder_hidden_dim=512, decoder_hidden_dim=512, dropout=0.5)\r\n",
    "    decode_layer = Decoder(output_dim=len(summary.vocab),emb_dim=256, enc_hid_dim=512, dec_hid_dim=512, dropout=0.5, attention=attention_layer)\r\n",
    "    model = Seq2Seq(encode_layer,decode_layer, article.vocab.stoi[article.pad_token], device).to(device)\r\n",
    "\r\n",
    "    sum_pad_ids = summary.vocab.stoi[summary.pad_token]\r\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = sum_pad_ids)\r\n",
    "\r\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\r\n",
    "    test_loss = evaluate(model, test_loader, criterion)\r\n",
    "    print(\"Loaded Best Model Info:\")\r\n",
    "    print(f'Test Loss: {test_loss:.3f} / Test PPL: {math.exp(test_loss):7.3f}')\r\n",
    "\r\n",
    "    return article,summary,model\r\n",
    "\r\n",
    "\r\n",
    "def predict(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
    "    model.eval()\r\n",
    "    if sentence is str:\r\n",
    "        nlp = spacy.load('en_core_web_trf')\r\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
    "    else:\r\n",
    "        tokens = [token.lower() for token in sentence]\r\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]        \r\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\r\n",
    "    src_len = torch.LongTensor([len(src_indexes)]).to(device)\r\n",
    "    with torch.no_grad():\r\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor, src_len.cpu())\r\n",
    "    mask = model.create_mask(src_tensor)        \r\n",
    "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
    "    attentions = torch.zeros(max_len, 1, len(src_indexes)).to(device)\r\n",
    "    for i in range(max_len):\r\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\r\n",
    "        with torch.no_grad():\r\n",
    "            output, hidden, attention = model.decoder(trg_tensor, hidden, encoder_outputs, mask)\r\n",
    "        attentions[i] = attention            \r\n",
    "        pred_token = output.argmax(1).item()        \r\n",
    "        trg_indexes.append(pred_token)\r\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
    "            break\r\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]\r\n",
    "\r\n",
    "def spacy_tokenizer(sentence):\r\n",
    "    tokens = parser(sentence)\r\n",
    "    tokens = [tok.lower_ for tok in tokens]\r\n",
    "    return tokens\r\n",
    "\r\n",
    "def predicttext(text,article,summary,model,device):\r\n",
    "    text_tokenized = spacy_tokenizer(text)\r\n",
    "    prediction, _ = predict(text_tokenized, article, summary, model, device)\r\n",
    "    return cleanoutput(prediction)\r\n",
    "    \r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def cleanoutput(prediction):\r\n",
    "    def changestarttoupper(string):\r\n",
    "        list1 = []\r\n",
    "        for i in string: list1.append(i)\r\n",
    "        list1[0] = list1[0].upper()\r\n",
    "        return \"\".join(list1)\r\n",
    "\r\n",
    "    curedprediction = [k for k, g in itertools.groupby(prediction)]\r\n",
    "    words_delete_index = []   # 存放要删除的重复字符出现的位置\r\n",
    "    for pos1 in range(len(curedprediction)):\r\n",
    "        pos1_number = curedprediction[pos1+1:].count(curedprediction[pos1])   # 当前字符在每一行中出现的次数\r\n",
    "        if pos1_number == 0:   # 如果当前字符在这一行中没有重复，则跳过\r\n",
    "            continue\r\n",
    "        else:\r\n",
    "            pos2 = pos1\r\n",
    "            for pos1_repeated_times in range(pos1_number):   # 对比每个重复出现的位置的下一个字符是否也是重复的\r\n",
    "                pos2 = curedprediction.index(curedprediction[pos1], pos2+1, len(curedprediction))   # 找到当前字符下一次出现的位置\r\n",
    "                if pos2 >= len(curedprediction)-1:   # 如果已经查询到这一行数据的最后一个字符，则跳过\r\n",
    "                    continue\r\n",
    "                else:\r\n",
    "                    if curedprediction[pos1+1] == curedprediction[pos2+1]:   # 判断当前字符的下一个字符是否与重复出现的字符的下一个字符相等\r\n",
    "                        words_delete_index.append(pos2)\r\n",
    "                        words_delete_index.append(pos2+1)\r\n",
    "                    else:\r\n",
    "                        continue\r\n",
    "    words_delete_index = list(set(words_delete_index))   # 去掉需要重复删除的索引\r\n",
    "    words_delete_index.sort(reverse=True)   # 对要删除的位置索引”从大到小“排列，方便后续删除操作\r\n",
    "    for delete_index in words_delete_index:\r\n",
    "        del curedprediction[delete_index]\r\n",
    "    del curedprediction[-1]\r\n",
    "    strtoreturn = curedprediction[0]\r\n",
    "    for i in curedprediction[1:]:\r\n",
    "        if i[0] in [\",\",\"'\"]:\r\n",
    "            strtoreturn+=i\r\n",
    "        elif i[0] in [\".\",\"\\\"\"]:\r\n",
    "            i[0] = changestarttoupper(i[0])\r\n",
    "            strtoreturn+=i\r\n",
    "        else:\r\n",
    "            strtoreturn+=(\" \"+i)\r\n",
    "    strtoreturn+=\".\"\r\n",
    "    \r\n",
    "    return changestarttoupper(strtoreturn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "text_tokenized = spacy_tokenizer(\"Reliance Industries' Chairman Mukesh Ambani's daughter Isha Ambani has featured on the cover of the February edition of Vogue India. She's dressed in a white shirt dress and black ruffled skirt by Australian designer Toni Maticevski, while accessorising her look with a Misho ring. In the cover story on her, Isha has spoken about her work and life after marriage.\")\r\n",
    "print(len(text_tokenized))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "print(\"正在加载模型...\\nLoading TextSUM model to Device: \", device)\r\n",
    "article,summary,model = load_best_model(device,model_path = model_path ,store_dict=\"./data\")\r\n",
    "print(\"模型加载成功！ The model is successfully initted!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "正在加载模型...\n",
      "Loading TextSUM model to Device:  cuda\n",
      "Loaded Best Model Info:\n",
      "Test Loss: 2.135 / Test PPL:   8.455\n",
      "模型加载成功！ The model is successfully initted!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "txt = \"Actor Shah Rukh Khan's manager Pooja Dadlani has said that the actor almost punching anchor Ramez Galal over a prank was staged. She added, He was aware and was acting as if he was angry. Ramez, inside a reptile-like costume, crawled towards Shah Rukh's vehicle to scare him, after it got stuck in quicksand, as part of the prank.\"\r\n",
    "\r\n",
    "print(predicttext(txt,article,summary,model,device))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Srk's anger on prank show was staged, says manager\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}